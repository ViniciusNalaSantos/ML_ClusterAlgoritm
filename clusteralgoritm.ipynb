{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/viniciusnalasantos/k-means-clustering?scriptVersionId=124367579\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Clustering: Extracting Patterns From Data & Concept + Feature Scaling + PCA [Pt.1]\n## Customers segmentation based on their credit card usage behavior\n\n![credit_cards.png](https://miro.medium.com/v2/resize:fit:828/0*S9zDFD_KE5yEiiaY)\n\nThis is the first article of a series in which I will outline an end-to-end clustering project. We will begin with some initial concepts, going to dataset exploratory analysis, passing through data preprocessing, grouping customers into clusters, and at the end, we will perform an analysis of the clusters, plus marketing suggestions.\n\n It's lots of things to do, so let's start!","metadata":{}},{"cell_type":"markdown","source":" ### Table of Contents\n\n[Part One](#part_one)\n* [Clustering Definition](#clustering_definition)\n* [Supervisionized Learning x Unsupervisionized Learning](#supervised_unsupervised)\n* [K-Means Clustering](#concept)\n* [Dataset Dictionary](#data_dictionary)\n* [Project Aim](#project_aim)\n* [EDA (Exploratory Data Analysis)](#eda)\n* [Feature Scaling](#feature_scaling)\n* [PCA (Principal Component Analysis)](#pca)\n* [Cluster Visualization with Plotly](#visualization)\n* [Conclusion](#conclu)\n\n[Part Two](#part_two)\n* [Coming soon...](#)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"part_one\"></a>\n<h2><b> Part One </b></h2>\n\n---\n\n#### Code from the article: [Clustering: Extracting Patterns From Data & Concept + Feature Scaling + PCA [Pt.1]](https://medium.com/@viniciusnala/clustering-extracting-patterns-from-data-concept-feature-scaling-pca-pt-1-4f33c6d39a71)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"clustering_definition\"></a>\n## Clustering Definition\n\nClustering or data grouping analysis is a set of data mining techniques that aims to make an automatic grouping of the data according to their degree of similarity. The criterion of similarity depends on the problem and algorithm. The result of this process is the division of a data set into a certain amount of groups (clusters).\n\nThis is a very important definition, and all that we will do after that is related somewhat to this definition. Another very important definition is the difference between the two concepts below.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"supervised_unsupervised\"></a>\n## Supervised Learning x Unsupervised Learning\n\nSupervised Learning is an approach where an algorithm is trained on input data that has been labeled for a particular output. The model fits the data capturing the underlying pattern and relationship between the input data and the output labels, enabling it to predict outcomes accurately.\n\nThe two anterior data science projects that I did:\n\n- [Machine Learning Model to Predict Survival in Titanic [Pt. 1]](https://medium.com/@viniciusnala/machine-learning-model-to-predict-survival-in-titanic-pt-1-b3681d1794fb)\n- [Linear Regression: Advanced Modeling Techniques & Pipeline [Pt.1]](https://medium.com/@viniciusnala/linear-regression-advanced-modeling-techniques-pipeline-pt-1-3c0433230b88)\n\nI built a supervised machine learning model, which I used to predict a \"target variable\" (label).\n\nFor example, in the first project, I constructed a model to predict if a person would survive or not on Titanic. To do that I provided data containing general characteristics of each passenger and the information if this person survived or not on Titanic (target variable) - the model captured the underlying pattern and relationship between the general characteristics of the passenger and if he survived or not -, then I presented to it neve-before-seen data with general characteristics of some passengers, but without the information if the passengers survived or not, being the model responsible to guess who would survive or not.\n\nUnlike, supervised learning, unsupervised learning uses unlabeled data, i.e. we don't provide the target variable (label) when training the model. These algorithms aren't used to predict something but to discover hidden patterns in data without the need for human intervention (hence, they are \"unsupervised\").\n\nIf the difference is still not so clear, along the project you will understand better. Let's continue!","metadata":{}},{"cell_type":"markdown","source":"<a id=\"concept\"></a>\n## K-Means Clustering\n\nThe goal of clustering is to identify meaningful groups of data. These groups can be analyzed in depth, or be passed as a feature, or as an outcome to a classification or regression model. K-Means was the first clustering method to be developed, has a very simple algorithm, and is still widely used.\n\n![clustering.png](https://miro.medium.com/v2/resize:fit:1100/format:webp/0*Cei6GQZkabRgn7J8.png)\n\nK-Means divides the data into a K number of clusters, each cluster has a centroid. The main aim of this algorithm is to minimize the sum of the squared distances of each point to the centroid of its assigned cluster. K-Means does not ensure the clusters will have the same size but finds the clusters that are best separated.\n\nLet's see a simple example, imagine that we have an N number of records and two variables X, and Y. Suppose we wanna split the data into K = 3 clusters, which means assigning each record (Xi, Yi) to a cluster K.\n\n![cluster_1.png](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*xA_xdOKC9-GA0xcbw77fog.png)\n\nThe algorithm first selects random coordinates for the centroids, and using the euclidean distance formula measures the distance between each data point and each cluster centroid. Then each point is assigned to the closest centroid.\n\n![distance.png](https://miro.medium.com/v2/resize:fit:600/format:webp/0*uvyEvT5qpoyv2cFX.png)\n\n![cluster_2.png](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ozKTU9kb_OgbU-QFA4myLA.png)\n\nAfter that, the coordinates of the centroids are re-computed by taking the mean of all data points contained in that cluster. Given an assignment of Nk records to cluster K, the center of the cluster(Xk, Yk) is calculated through the equation:\n\n![equation_1.png](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ZCZ97gHVIh2j1FExJzXpow.png)\n\nIn simple terms, we are just summing the Xk and Yk of the cluster and dividing by the number of points in that cluster.\n\nAfter this process, the algorithm computes the sum of squares within each cluster, which is given by the formula:\n\n![equation_2.png](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Te9NFMLZ9v5ZnIl_Zsp7Tg.png)\n\nThe K-Means keep repeating the process of measuring the distance between the points and the centroids, assigning each data point to the closest center, and re-computing the new coordinates of the centroids until the sum of squares across all three clusters is minimized.\n\n![equation_3.png](https://miro.medium.com/v2/resize:fit:640/format:webp/1*odKVjftJNYpoZ9iJhudejA.png)\n\n![cluster_3.png](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*t_cti7a3DasJ9BT1YpOTmg.png)\n\nRepeat the same process many times can require a great computational cost, especially when the amount of data is large. \n\nIf you wanna see by yourself how it works, I highly recommend this simulator: [K-Means Clustering Simulator](https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/).","metadata":{}},{"cell_type":"markdown","source":"<a id=\"data_dictionary\"></a>\n## Dataset Dictionary\n\nThe dataset that will be used consists of the credit card usage behavior of 8950 customers during 6 months, having 18 behavioral features. The link to the dataset is available on Kaggle.\n\n[Credit Card Dataset for Clustering](https://www.kaggle.com/datasets/arjunbhasin2013/ccdata)\n\n- CUST_ID: Identification of Credit Card holder (Categorical)\n- BALANCE: Balance amount left in their account to make purchases\n- BALANCE_FREQUENCY: How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n- PURCHASES: Amount of purchases made from account\n- ONEOFF_PURCHASES: Maximum purchase amount done in one-go\n- INSTALLMENTS_PURCHASES: Amount of purchase done in installment\n- CASH_ADVANCE: Cash in advance given by the user\n- PURCHASES_FREQUENCY: How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n- ONEOFFPURCHASESFREQUENCY : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n- PURCHASESINSTALLMENTSFREQUENCY: How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n- CASHADVANCEFREQUENCY: How frequently the cash in advance being paid\n- CASHADVANCETRX: Number of Transactions made with \"Cash in Advanced\"\n- PURCHASES_TRX: Number of purchase transactions made\n- CREDIT_LIMIT: Limit of Credit Card for user\n- PAYMENTS: Amount of Payment done by user\n- MINIMUM_PAYMENTS: Minimum amount of payments made by user\n- PRCFULLPAYMENT: Percent of full payment paid by user\n- TENURE: Tenure of credit card service for user","metadata":{}},{"cell_type":"markdown","source":"<a id=\"project_aim\"></a>\n## Project Aim\n\nInitially, our focus will be to segment customers according to their similarities. After that, we will analyze this segmentation and define an effective credit card marketing strategy.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"eda\"></a>\n## EDA (Exploratory Data Analysis)\n\nLet's begin by importing the libraries and the dataset:","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-02T00:58:02.714924Z","iopub.execute_input":"2023-04-02T00:58:02.715354Z","iopub.status.idle":"2023-04-02T00:58:02.744098Z","shell.execute_reply.started":"2023-04-02T00:58:02.715318Z","shell.execute_reply":"2023-04-02T00:58:02.743167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-04-01T23:38:43.292042Z","iopub.execute_input":"2023-04-01T23:38:43.292561Z","iopub.status.idle":"2023-04-01T23:38:43.299279Z","shell.execute_reply.started":"2023-04-01T23:38:43.292512Z","shell.execute_reply":"2023-04-01T23:38:43.297465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/ccdata/CC GENERAL.csv')","metadata":{"execution":{"iopub.status.busy":"2023-04-01T23:38:48.652679Z","iopub.execute_input":"2023-04-01T23:38:48.653984Z","iopub.status.idle":"2023-04-01T23:38:48.727352Z","shell.execute_reply.started":"2023-04-01T23:38:48.653919Z","shell.execute_reply":"2023-04-01T23:38:48.725599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualizing some characteristics of the dataset:","metadata":{}},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T23:38:53.770669Z","iopub.execute_input":"2023-04-01T23:38:53.771131Z","iopub.status.idle":"2023-04-01T23:38:53.817734Z","shell.execute_reply.started":"2023-04-01T23:38:53.771092Z","shell.execute_reply":"2023-04-01T23:38:53.81629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-01T23:40:08.721011Z","iopub.execute_input":"2023-04-01T23:40:08.721641Z","iopub.status.idle":"2023-04-01T23:40:08.729621Z","shell.execute_reply.started":"2023-04-01T23:40:08.721584Z","shell.execute_reply":"2023-04-01T23:40:08.728337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Through Pandas Profiling we can see a very detailed report about the general characteristics of the data set.","metadata":{}},{"cell_type":"code","source":"from pandas_profiling import ProfileReport\n\nProfileReport(df).to_notebook_iframe()","metadata":{"execution":{"iopub.status.busy":"2023-04-01T23:42:54.330892Z","iopub.execute_input":"2023-04-01T23:42:54.33138Z","iopub.status.idle":"2023-04-01T23:44:32.640865Z","shell.execute_reply.started":"2023-04-01T23:42:54.33134Z","shell.execute_reply":"2023-04-01T23:44:32.638928Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dowload the report\nfrom pandas_profiling import ProfileReport\n\nprofile = ProfileReport(df, title=\"Pandas Profiling Report\")\nprofile.to_file(\"report.html\")","metadata":{"execution":{"iopub.status.busy":"2023-04-01T23:53:47.126999Z","iopub.execute_input":"2023-04-01T23:53:47.127401Z","iopub.status.idle":"2023-04-01T23:55:15.490893Z","shell.execute_reply.started":"2023-04-01T23:53:47.127368Z","shell.execute_reply":"2023-04-01T23:55:15.489533Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After analyzing the report I concluded that the features CUSTOMER_ID and TENURE are not going to contribute with the segmentation so we can exclude them from the dataset.","metadata":{}},{"cell_type":"code","source":"# Delete the irrelevant features\ndf.drop(columns=['CUST_ID', 'TENURE'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-02T00:07:07.261239Z","iopub.execute_input":"2023-04-02T00:07:07.261657Z","iopub.status.idle":"2023-04-02T00:07:07.269154Z","shell.execute_reply.started":"2023-04-02T00:07:07.261621Z","shell.execute_reply":"2023-04-02T00:07:07.267639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now our dataset has 16 columns.\n\nLet's see how many missing values there are in the dataset.","metadata":{}},{"cell_type":"code","source":"# See Null Values\ndf.isnull().sum().sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-02T01:32:19.222696Z","iopub.execute_input":"2023-04-02T01:32:19.223169Z","iopub.status.idle":"2023-04-02T01:32:19.23515Z","shell.execute_reply.started":"2023-04-02T01:32:19.223131Z","shell.execute_reply":"2023-04-02T01:32:19.233792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just a few ones, to deal with them I will use KNNImputer: each missing value will be imputed using the mean value from the n_neighbors nearest neighbors, in this case, will be the mean value of the 5 nearest neighbors.","metadata":{"execution":{"iopub.status.busy":"2023-04-02T00:07:10.278144Z","iopub.execute_input":"2023-04-02T00:07:10.27858Z","iopub.status.idle":"2023-04-02T00:07:10.290739Z","shell.execute_reply.started":"2023-04-02T00:07:10.27854Z","shell.execute_reply":"2023-04-02T00:07:10.28933Z"}}},{"cell_type":"code","source":"from sklearn.impute import KNNImputer\n\n# Columns with missing values\nnull_columns = df.columns[df.isnull().any()].tolist()\n\n# Fill Null values\nimputer = KNNImputer(n_neighbors=5)\ndf_imp = pd.DataFrame(imputer.fit_transform(df[null_columns]), columns=null_columns)\ndf = df.fillna(df_imp)","metadata":{"execution":{"iopub.status.busy":"2023-04-02T00:10:49.121235Z","iopub.execute_input":"2023-04-02T00:10:49.121689Z","iopub.status.idle":"2023-04-02T00:10:49.771765Z","shell.execute_reply.started":"2023-04-02T00:10:49.121654Z","shell.execute_reply":"2023-04-02T00:10:49.770244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This way, instead of filling all the values with the mean or the median, which is considered a simple approach, we reduce the risk of biasing the clustering results.","metadata":{}},{"cell_type":"code","source":"# Distribution Visualization\nplt.figure(figsize=(20,35))\n\nfor i, col in enumerate(df.columns):\n    ax = plt.subplot(9, 2, i+1)\n    sns.kdeplot(df[col], ax=ax)\n    plt.xlabel(col)\n        \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-02T00:16:01.41374Z","iopub.execute_input":"2023-04-02T00:16:01.414203Z","iopub.status.idle":"2023-04-02T00:16:07.520494Z","shell.execute_reply.started":"2023-04-02T00:16:01.414167Z","shell.execute_reply":"2023-04-02T00:16:07.519048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Almost every variable is very right-skewed or very left-skewed, this indicates that maybe there are some outliers, which is not something very surprising since we are working with credit card. Certainly, there will be a small portion of people that have a very high amount of money and credit limit, while the majority portion of people has more or less the same amount of money and credit limit.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"feature_scaling\"></a>\n## Feature Scaling\n\n(photo)\n\nIf you see the frequency columns like BALANCE_FREQUENCY, and PURCHASES_FREQUENCY, they vary between an interval of 0 and 1, 0 means 0% of frequency and 1 means 100% frequency. Comparing them with the columns BALANCE and PURCHASES, you will see that they don't have a limit to their variation, we only know that their minimum can be 0.\n\nHowever, if you put these columns in the way they're right now in the cluster, it will not yield a high-quality cluster because the cluster will understand that 1.00 dollar difference in BALANCE is as significant as 1.00 percent difference in BALANCE_FREQUENCY.\n \nIn virtue of that, we need to put all the columns into the same scale, otherwise, will be the same thing as clustering people on their weights in kilograms and heights in meters, is a 1kg difference as significant as a 1m difference in height?\n \nThis is why scaling the dataset is a vital part when using clustering techniques. Therefore, we will use Normalizer to scale the dataset, but there are many ways to scale a dataset, and each one is used for a specific situation.","metadata":{}},{"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import Normalizer\n\nscaler = Normalizer(norm='l2')","metadata":{"execution":{"iopub.status.busy":"2023-04-02T00:20:25.735527Z","iopub.execute_input":"2023-04-02T00:20:25.73602Z","iopub.status.idle":"2023-04-02T00:20:25.74169Z","shell.execute_reply.started":"2023-04-02T00:20:25.735975Z","shell.execute_reply":"2023-04-02T00:20:25.740678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Normalizer, unlike the other methods, works on the rows, not the columns. This seems very unintuitive, but it means that will scale each value according to its line, and not according to the values in its column.\n\nBy default, L2 normalization is applied to each observation so the values in a row have a unit norm. Unit norm with L2 means that if each element were squared and summed, the total would equal 1. Moreover, Normalizer transforms all the features into values between -1 and 1.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"pca\"></a>\n## PCA (Principal Component Analysis)","metadata":{}},{"cell_type":"code","source":"# Reduce dimensions\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2, random_state=1234)","metadata":{"execution":{"iopub.status.busy":"2023-04-02T00:20:29.081283Z","iopub.execute_input":"2023-04-02T00:20:29.081702Z","iopub.status.idle":"2023-04-02T00:20:29.088425Z","shell.execute_reply.started":"2023-04-02T00:20:29.081664Z","shell.execute_reply":"2023-04-02T00:20:29.086693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PCA is a method used in unsupervised machine learning (such as clustering) that reduces high-dimension data to smaller dimensions while preserving as much information as possible. Using PCA before applying clustering algorithm reduces dimensions, data noise, and decreases computation cost.\n\nIn this article, the number of features will be reduced to 2 dimensions so that the clustering results can be visualized.\n\n\n                                                                        ...\nTo organize better these two preprocessing steps we will embed them into a unique step with Pipeline:","metadata":{}},{"cell_type":"code","source":"# Preprocessing\nfrom sklearn.pipeline import Pipeline\n\n# Pipeline\npipe_normalizedscaled_pca = Pipeline([('scaling', scaler), ('pca', pca)])","metadata":{"execution":{"iopub.status.busy":"2023-04-02T00:21:00.881637Z","iopub.execute_input":"2023-04-02T00:21:00.882956Z","iopub.status.idle":"2023-04-02T00:21:00.898214Z","shell.execute_reply.started":"2023-04-02T00:21:00.882892Z","shell.execute_reply":"2023-04-02T00:21:00.896674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset will be in this way after the preprocessing:","metadata":{}},{"cell_type":"code","source":"# Transformed dataset\npd.DataFrame(\n    pipe_normalizedscaled_pca.fit_transform(df),\n    columns=['x', 'y']\n)","metadata":{"execution":{"iopub.status.busy":"2023-04-02T00:21:12.337342Z","iopub.execute_input":"2023-04-02T00:21:12.337821Z","iopub.status.idle":"2023-04-02T00:21:12.410055Z","shell.execute_reply.started":"2023-04-02T00:21:12.33775Z","shell.execute_reply":"2023-04-02T00:21:12.408744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"visualization\"></a>\n## Cluster Visualization with Plotly\n\nOnce done the preprocessing, the only thing that remains is the modeling and visualization. And to do this I will create a function, so we can use it whenever we want.","metadata":{}},{"cell_type":"code","source":"# Libs\nfrom sklearn.cluster import KMeans\nimport plotly.express as px\nimport seaborn as sns\n\n# Function\ndef Visualize_Cluster(df, pipeline, n_clusters):\n    '''\n    Display a scatter plot cluster after transforming the data and using it to fit KMeans Cluster \n    \n        Parameters:\n                df (pandas.core.frame.DataFrame): Dataframe that will be used in the Pipeline and train the KMeans Cluster\n                pipeline (sklearn.pipeline.Pipeline): Transform the Dataframe\n                n_clusters (int): Number of clusters that the KMeans Cluster will have\n        \n        Returns:\n                None    \n    '''\n    \n    data = pd.DataFrame(pipeline.fit_transform(df), columns=['x', 'y'])\n    \n    kmeans = KMeans(n_clusters=n_clusters, n_init=10, max_iter=300, verbose=False, random_state=1234)\n    clusters = pd.DataFrame(kmeans.fit_predict(data), columns=['Cluster']) \n    \n    clusters_data = pd.concat([data, clusters], axis=1)\n    \n    fig = px.scatter(clusters_data, x='x', y='y', color='Cluster')\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-02T00:22:39.640001Z","iopub.execute_input":"2023-04-02T00:22:39.641344Z","iopub.status.idle":"2023-04-02T00:22:42.381739Z","shell.execute_reply.started":"2023-04-02T00:22:39.641297Z","shell.execute_reply":"2023-04-02T00:22:42.380437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, we import the KMeans library for the clustering and the Plotly library for the cluster visualization. Then we write the function that has as parameters: the dataset for training the KMeans model, the pipeline for preprocessing, and the number of clusters in KMeans.\n\nInternally, the function will transform the dataset according to the steps in the Pipeline, assuming that this transformation will only return two columns, which will be named \"x\", and \"y\". Create the KMeans Cluster, and inside this object, it will be passed the number of clusters previously specified in the parameters of the function as an argument, so we can visualize how many clusters we want. And in the end, we will plot the scatter graph of the clustering, each color will be a cluster.\n\nNow, let's visualize our clustering with 5 clusters.","metadata":{}},{"cell_type":"code","source":"Visualize_Cluster(df, pipe_normalizedscaled_pca, n_clusters=5)","metadata":{"execution":{"iopub.status.busy":"2023-04-02T00:22:49.429948Z","iopub.execute_input":"2023-04-02T00:22:49.430381Z","iopub.status.idle":"2023-04-02T00:22:52.638239Z","shell.execute_reply.started":"2023-04-02T00:22:49.430342Z","shell.execute_reply":"2023-04-02T00:22:52.637003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remember: almost all attributes of the KMeans model in the function are with the default values, the only attribute that we can change is the number of clusters of the clustering. Therefore, we can make the clustering even better by changing the parameters of the KMeans model.\n\nLet's see how would be our clustering with 10 clusters.","metadata":{}},{"cell_type":"code","source":"Visualize_Cluster(df, pipe_normalizedscaled_pca, n_clusters=10)","metadata":{"execution":{"iopub.status.busy":"2023-03-27T16:18:02.538001Z","iopub.execute_input":"2023-03-27T16:18:02.539192Z","iopub.status.idle":"2023-03-27T16:18:04.12478Z","shell.execute_reply.started":"2023-03-27T16:18:02.539126Z","shell.execute_reply":"2023-03-27T16:18:04.123278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"conclu\"></a>\n## Conclusion\n\nIn this first part, we started the project learning some concepts and understanding how clustering works. In part two of the series, we will give continuity to the project by learning the most common metrics used to validate a cluster and how to use these metrics to find the ideal number of clusters for this dataset.\n\nIn virtue of the content being about metrics, we will have to enter into math equations, so I will explain many things in mathematical terms (what people usually dislike, but the subject being studied requires that be this way), and I will assume that you have at least a basic algebra knowledge.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"part_two\"></a>\n<h2><b> Part Two </b></h2>\n\n---\n\n#### Coming soon...","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n\npipe_normalizer = Pipeline([('normalization', Normalizer())])","metadata":{"execution":{"iopub.status.busy":"2023-03-21T11:25:24.856328Z","iopub.execute_input":"2023-03-21T11:25:24.856821Z","iopub.status.idle":"2023-03-21T11:25:24.864871Z","shell.execute_reply.started":"2023-03-21T11:25:24.856782Z","shell.execute_reply":"2023-03-21T11:25:24.862757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pipe_normalizer.fit_transform(df)\ndata = pd.DataFrame(data, columns=df.columns)\n\ndef cluster_algorithm(n_clusters, dataset):\n    kmean = KMeans(n_clusters=n_clusters, random_state=1234)\n    labels = kmean.fit_predict(dataset)\n    \n    si = silhouette_score(dataset, labels, metric='euclidean', random_state=1234)\n    db = davies_bouldin_score(dataset, labels)\n    ch = calinski_harabasz_score(dataset, labels)\n    \n    return si, db, ch\n\n\nn_clusters, si_list, db_list, ch_list = list(), list(), list(), list()\n\nfor i in range(2, 101):\n    si, db, ch = cluster_algorithm(i, data)\n    n_clusters.append(int(i))\n    si_list.append(si)\n    db_list.append(db)\n    ch_list.append(ch)\n\nmetrics = pd.DataFrame(np.column_stack((n_clusters, si_list, db_list, ch_list)), columns=['N° Clusters', 'silhouette', 'davies_bouldin', 'calinski_harabasz'])","metadata":{"execution":{"iopub.status.busy":"2023-03-21T11:29:02.380216Z","iopub.execute_input":"2023-03-21T11:29:02.380728Z","iopub.status.idle":"2023-03-21T11:39:12.488706Z","shell.execute_reply.started":"2023-03-21T11:29:02.380661Z","shell.execute_reply":"2023-03-21T11:39:12.486649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\n# Silhouette\nfig = px.line(metrics, x='N° Clusters', y='silhouette', title='Silhouette')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T11:39:38.329252Z","iopub.execute_input":"2023-03-21T11:39:38.329749Z","iopub.status.idle":"2023-03-21T11:39:38.433106Z","shell.execute_reply.started":"2023-03-21T11:39:38.329707Z","shell.execute_reply":"2023-03-21T11:39:38.431717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.line(metrics, x='N° Clusters', y='davies_bouldin', title='Davies Bouldin')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T11:39:59.862197Z","iopub.execute_input":"2023-03-21T11:39:59.862694Z","iopub.status.idle":"2023-03-21T11:39:59.926712Z","shell.execute_reply.started":"2023-03-21T11:39:59.862653Z","shell.execute_reply":"2023-03-21T11:39:59.925238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.line(metrics, x='N° Clusters', y='calinski_harabasz', title='Calinski Harabasz')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T11:40:26.077332Z","iopub.execute_input":"2023-03-21T11:40:26.077856Z","iopub.status.idle":"2023-03-21T11:40:26.142328Z","shell.execute_reply.started":"2023-03-21T11:40:26.077815Z","shell.execute_reply":"2023-03-21T11:40:26.141039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics[metrics['N° Clusters'] <= 8][['N° Clusters', 'silhouette']].sort_values(by='silhouette', ascending=False)\n\nmetrics[metrics['N° Clusters'] <= 8][['N° Clusters', 'davies_bouldin']].sort_values(by='davies_bouldin', ascending=True)\n\nmetrics[metrics['N° Clusters'] <= 8][['N° Clusters', 'calinski_harabasz']].sort_values(by='calinski_harabasz', ascending=False)\n\n# n_cluster = 5\nmetrics[metrics['N° Clusters'] == 5].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T11:41:21.987783Z","iopub.execute_input":"2023-03-21T11:41:21.989217Z","iopub.status.idle":"2023-03-21T11:41:22.011348Z","shell.execute_reply.started":"2023-03-21T11:41:21.989162Z","shell.execute_reply":"2023-03-21T11:41:22.010286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = np.random.rand(8950, 16)\n\nsi, db, ch = cluster_algorithm(5, dataset)\n\npd.DataFrame(data=[[5, si, db, ch]], columns=['N° Clusters', 'silhouette', 'davies_bouldin', 'calinsk_harabasz'])","metadata":{"execution":{"iopub.status.busy":"2023-03-21T11:42:53.691993Z","iopub.execute_input":"2023-03-21T11:42:53.693175Z","iopub.status.idle":"2023-03-21T11:42:58.511839Z","shell.execute_reply.started":"2023-03-21T11:42:53.693112Z","shell.execute_reply":"2023-03-21T11:42:58.509835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Metrics of the cluster with a normal dataset:')\ndisplay(metrics[metrics['N° Clusters'] == 5].reset_index(drop=True))\nprint('Metrics of the cluster with a random dataset:')\ndisplay(pd.DataFrame(data=[[5, si, db, ch]], columns=['N° Clusters', 'silhouette', 'davies_bouldin', 'calinsk_harabasz']))","metadata":{"execution":{"iopub.status.busy":"2023-03-21T11:43:07.454862Z","iopub.execute_input":"2023-03-21T11:43:07.455343Z","iopub.status.idle":"2023-03-21T11:43:07.478016Z","shell.execute_reply.started":"2023-03-21T11:43:07.455302Z","shell.execute_reply":"2023-03-21T11:43:07.476742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"splited_df = np.array_split(df, 5)\n\nsection, si_list, db_list, ch_list = list(), list(), list(), list()\n\nfor i in range(5):\n    si, sb, ch = cluster_algorithm(5, splited_df[i])\n    section.append(i+1)\n    si_list.append(si)\n    db_list.append(sb)\n    ch_list.append(ch)\n\n\npd.DataFrame(np.column_stack([section, si_list, db_list, ch_list]), columns=['Section N°', 'silhouette', 'davies_bouldin', 'calinski_harabasz'])","metadata":{"execution":{"iopub.status.busy":"2023-03-21T11:43:28.161956Z","iopub.execute_input":"2023-03-21T11:43:28.162463Z","iopub.status.idle":"2023-03-21T11:43:35.565312Z","shell.execute_reply.started":"2023-03-21T11:43:28.162424Z","shell.execute_reply":"2023-03-21T11:43:35.56285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data = pd.DataFrame(pipe_normalizer.fit_transform(df), columns=df.columns)\n    \nkmeans = KMeans(n_clusters=5, n_init=10, max_iter=300, verbose=False, random_state=1234)\nlabels = pd.DataFrame(kmeans.fit_predict(data), columns=['CLUSTER']) \n    \nclusters_data = pd.concat([df, labels], axis=1)\nclusters_data","metadata":{"execution":{"iopub.status.busy":"2023-03-21T11:47:18.581571Z","iopub.execute_input":"2023-03-21T11:47:18.582109Z","iopub.status.idle":"2023-03-21T11:47:20.136702Z","shell.execute_reply.started":"2023-03-21T11:47:18.582066Z","shell.execute_reply":"2023-03-21T11:47:20.135251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clusters_data['CLUSTER'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T11:47:46.990296Z","iopub.execute_input":"2023-03-21T11:47:46.990799Z","iopub.status.idle":"2023-03-21T11:47:47.004167Z","shell.execute_reply.started":"2023-03-21T11:47:46.990744Z","shell.execute_reply":"2023-03-21T11:47:47.002226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.scatter(clusters_data, x='PURCHASES', y='PAYMENTS', color='CLUSTER')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T11:48:46.391082Z","iopub.execute_input":"2023-03-21T11:48:46.39158Z","iopub.status.idle":"2023-03-21T11:48:46.470137Z","shell.execute_reply.started":"2023-03-21T11:48:46.391539Z","shell.execute_reply":"2023-03-21T11:48:46.468746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.scatter(clusters_data, x='PURCHASES', y='CASH_ADVANCE', color='CLUSTER')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T11:49:07.238777Z","iopub.execute_input":"2023-03-21T11:49:07.240206Z","iopub.status.idle":"2023-03-21T11:49:07.311076Z","shell.execute_reply.started":"2023-03-21T11:49:07.240151Z","shell.execute_reply":"2023-03-21T11:49:07.309828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(clusters_data, hue='CLUSTER', palette='bright')","metadata":{"execution":{"iopub.status.busy":"2023-03-21T11:49:31.485892Z","iopub.execute_input":"2023-03-21T11:49:31.486389Z","iopub.status.idle":"2023-03-21T11:54:17.876661Z","shell.execute_reply.started":"2023-03-21T11:49:31.486347Z","shell.execute_reply":"2023-03-21T11:54:17.874842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clusters_data.groupby(by='CLUSTER').describe()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:04:26.214189Z","iopub.execute_input":"2023-03-21T12:04:26.214756Z","iopub.status.idle":"2023-03-21T12:04:26.442523Z","shell.execute_reply.started":"2023-03-21T12:04:26.214706Z","shell.execute_reply":"2023-03-21T12:04:26.44109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"centroids = kmeans.cluster_centers_\ncentroids","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:13:44.615268Z","iopub.execute_input":"2023-03-21T12:13:44.61582Z","iopub.status.idle":"2023-03-21T12:13:44.626603Z","shell.execute_reply.started":"2023-03-21T12:13:44.615747Z","shell.execute_reply":"2023-03-21T12:13:44.625169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(centroids.shape[1]):\n    print(f'{clusters_data.columns[i]}:', '{:.4f}'.format(centroids[:, i].var()))","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:29:31.300021Z","iopub.execute_input":"2023-03-21T12:29:31.300506Z","iopub.status.idle":"2023-03-21T12:29:31.31071Z","shell.execute_reply.started":"2023-03-21T12:29:31.300457Z","shell.execute_reply":"2023-03-21T12:29:31.309079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"centroids_list = list()\n\nfor i in range(centroids.shape[1]):\n    centroids_list.append(centroids[:, i].var())\n\n\ncentroids_var = pd.DataFrame(centroids_list, index=clusters_data.columns[:-1], columns=['Variance of the Centroids'])\ncentroids_var","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:38:49.296067Z","iopub.execute_input":"2023-03-21T12:38:49.296464Z","iopub.status.idle":"2023-03-21T12:38:49.312898Z","shell.execute_reply.started":"2023-03-21T12:38:49.296431Z","shell.execute_reply":"2023-03-21T12:38:49.311413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"significant_centroids_var = centroids_var[centroids_var['Variance of the Centroids'] > 0.009]\nsignificant_centroids_var","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:46:05.508744Z","iopub.execute_input":"2023-03-21T12:46:05.50924Z","iopub.status.idle":"2023-03-21T12:46:05.523923Z","shell.execute_reply.started":"2023-03-21T12:46:05.509201Z","shell.execute_reply":"2023-03-21T12:46:05.521878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import stats\n\nstats.probplot(clusters_data['BALANCE'], plot=plt)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:16:18.270258Z","iopub.execute_input":"2023-03-21T13:16:18.27075Z","iopub.status.idle":"2023-03-21T13:16:18.559263Z","shell.execute_reply.started":"2023-03-21T13:16:18.270711Z","shell.execute_reply":"2023-03-21T13:16:18.557896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset is a usage of account during the last 6 months so it means that at the start of these 6 months, lets say the amount in account is VALUE\nwhere: VALUE = PURCHASE + BALANCE\nso PURCHASE is the amount of money the account spent\nand the BALANCE is the money left in VALUE","metadata":{}},{"cell_type":"code","source":"balance = significant_centroids_var.index[0]\n\nclusters_data[[balance, 'CLUSTER']].groupby(by='CLUSTER').describe()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T13:01:07.167329Z","iopub.execute_input":"2023-03-21T13:01:07.16783Z","iopub.status.idle":"2023-03-21T13:01:07.224054Z","shell.execute_reply.started":"2023-03-21T13:01:07.16776Z","shell.execute_reply":"2023-03-21T13:01:07.222074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"purchases = significant_centroids_var.index[1]\n\nclusters_data[[purchases, 'CLUSTER']].groupby(by='CLUSTER').describe()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:55:43.265742Z","iopub.execute_input":"2023-03-21T12:55:43.267246Z","iopub.status.idle":"2023-03-21T12:55:43.320388Z","shell.execute_reply.started":"2023-03-21T12:55:43.267172Z","shell.execute_reply":"2023-03-21T12:55:43.318852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cash_advance = significant_centroids_var.index[2]\n\nclusters_data[[cash_advance, 'CLUSTER']].groupby(by='CLUSTER').describe()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:56:26.128903Z","iopub.execute_input":"2023-03-21T12:56:26.129323Z","iopub.status.idle":"2023-03-21T12:56:26.189029Z","shell.execute_reply.started":"2023-03-21T12:56:26.129286Z","shell.execute_reply":"2023-03-21T12:56:26.187247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"credit_limit = significant_centroids_var.index[3]\n\nclusters_data[[credit_limit, 'CLUSTER']].groupby(by='CLUSTER').describe()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:57:31.646862Z","iopub.execute_input":"2023-03-21T12:57:31.647691Z","iopub.status.idle":"2023-03-21T12:57:31.702395Z","shell.execute_reply.started":"2023-03-21T12:57:31.647621Z","shell.execute_reply":"2023-03-21T12:57:31.700967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"payments = significant_centroids_var.index[4]\n\nclusters_data[[payments, 'CLUSTER']].groupby(by='CLUSTER').describe()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T16:37:11.077022Z","iopub.execute_input":"2023-03-21T16:37:11.077473Z","iopub.status.idle":"2023-03-21T16:37:11.174232Z","shell.execute_reply.started":"2023-03-21T16:37:11.077417Z","shell.execute_reply":"2023-03-21T16:37:11.172538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clusters_data[['PRC_FULL_PAYMENT', 'CLUSTER']].groupby(by='CLUSTER').describe()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:46:18.637034Z","iopub.execute_input":"2023-03-21T14:46:18.637535Z","iopub.status.idle":"2023-03-21T14:46:18.697923Z","shell.execute_reply.started":"2023-03-21T14:46:18.637495Z","shell.execute_reply":"2023-03-21T14:46:18.69559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"minimum_payments = significant_centroids_var.index[5]\n\nclusters_data[[minimu_payments, 'CLUSTER']].groupby(by='CLUSTER').describe()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T12:59:18.371343Z","iopub.execute_input":"2023-03-21T12:59:18.37188Z","iopub.status.idle":"2023-03-21T12:59:18.432907Z","shell.execute_reply.started":"2023-03-21T12:59:18.371841Z","shell.execute_reply":"2023-03-21T12:59:18.430714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clusters_data['CLUSTER'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:48:21.581342Z","iopub.execute_input":"2023-03-21T14:48:21.582253Z","iopub.status.idle":"2023-03-21T14:48:21.595212Z","shell.execute_reply.started":"2023-03-21T14:48:21.582201Z","shell.execute_reply":"2023-03-21T14:48:21.59342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}